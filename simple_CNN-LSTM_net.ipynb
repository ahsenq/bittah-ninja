{"cells":[{"source":["import argparse\n","import os\n","import pickle\n","import sys\n","from collections import Counter\n","\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from keras.layers import ConvLSTM2D, BatchNormalization, Dense, Flatten, Dropout\n","from keras.models import Sequential, Model\n","from keras.utils import Sequence\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","\n","try:\n","    os.chdir(os.path.join(os.getcwd(), 'FinalProject/bittah-ninja'))\n","    print(os.getcwd())\n","except:\n","    pass\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["parser = argparse.ArgumentParser()\n","parser.add_argument('-p', '--path', help='path to video set')\n","parser.add_argument('-e', '--epochs', help='number of epochs')\n","parser.add_argument('-b', '--batch_size', help='size of the batch')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["try:\n","    args = parser.parse_args()\n","except:\n","    pass\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["labelPath = 'first_1k_labeled.csv'\n","# labelPath = 'full_labels.csv'\n","df = pd.read_csv(labelPath)\n","df['label'] = df['class']\n","df.drop(columns=['class'], inplace=True)\n","df.groupby('label').size()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df = df.loc[df.label != -1]\n","df.groupby('label').size()"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df['punch'] = (df.label != 0).astype('int')\n","df.groupby('punch').size()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["if args.path:\n","    vidPath = args.path\n","else:\n","    vidPath = '../fullVidSet'\n","filenames = [os.path.join(vidPath, f) for f in df.clip_title]\n","labels = df.punch.tolist()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def getMaxFrameCount(filenames):\n","    frameCount = []\n","    for file in tqdm(filenames):\n","        cap = cv2.VideoCapture(file)\n","        frameCount.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n","\n","    return max(frameCount)\n","\n","\n","class DataGenerator(Sequence):\n","\n","    def __init__(self,\n","                 filenames,\n","                 labels,\n","                 batch_size,\n","                 max_frame_count,\n","                 frame_height=224,\n","                 frame_width=224,\n","                 n_channels=1):\n","        self.filenames = filenames\n","        self.labels = labels\n","        self.batch_size = batch_size\n","        self.max_frame_count = max_frame_count\n","        self.h = frame_height\n","        self.w = frame_width\n","        self.n_channels = n_channels\n","\n","    def __len__(self):\n","        return np.floor(len(self.filenames) / self.batch_size).astype(int)\n","\n","    def __data_generation(self, idx_list):\n","        def padEmptyFrames(vid):\n","            num_frames = self.max_frame_count - vid.shape[0]\n","            empty_frames = np.empty(\n","                (num_frames, vid.shape[1], vid.shape[2]), dtype=np.float16)\n","            padded_vid = np.vstack((vid, empty_frames)).astype(np.float16)\n","\n","            return padded_vid\n","\n","        def getFrames(filepath, pad_frames=True):\n","            cap = cv2.VideoCapture(filepath)\n","            vid = []\n","            while cap.isOpened():\n","                ret, frame = cap.read()\n","                if ret:\n","                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","                    # can try going bigger up to 720*1080 later\n","                    gray = cv2.resize(gray, (self.h, self.w))\n","                    vid.append(gray)\n","                    if cv2.waitKey(1) & 0xFF == ord('q'):\n","                        break\n","                else:\n","                    break\n","\n","            cap.release()\n","            vid = np.array(vid)\n","            if pad_frames:\n","                if vid.shape[0] < self.max_frame_count:\n","                    vid = padEmptyFrames(vid)\n","\n","            return vid\n","\n","        x = np.empty((self.batch_size,\n","                      self.max_frame_count,\n","                      self.w,\n","                      self.h,\n","                      self.n_channels), dtype=np.float16)\n","        y = np.empty((self.batch_size), dtype=np.float16)\n","        for i, idx in enumerate(idx_list):\n","            file = self.filenames[idx]\n","            vid = getFrames(file)\n","            vid = vid.reshape(self.max_frame_count,\n","                              self.w,\n","                              self.h,\n","                              self.n_channels)\n","            x[i, ] = vid\n","            y[i, ] = self.labels[idx]\n","            # y = tf.keras.utils.to_categorical(y,\n","            #                                   num_classes=self.n_classes,\n","            #                                   dtype='float16')\n","        # print(x.shape, y.shape)\n","        return x, y\n","\n","    def __getitem__(self, idx):\n","        batch = range(idx * self.batch_size, (idx + 1) * self.batch_size)\n","        x, y = self.__data_generation(batch)\n","        return x, y\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["if args.batch_size:\n","    batch_size = args.batch_size\n","else:\n","    batch_size = 10\n","# TODO: re-examine with full dataset\n","class_weight = {\n","    0: 0.33,\n","    1: 0.67\n","}\n","frame_height = 64\n","frame_width = 64\n","# frame_height = 224\n","# frame_width = 224\n","n_channels = 1"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["x_train, x_test, y_train, y_test = train_test_split(\n","    filenames, labels, test_size=0.2)\n","print(len(x_train), len(y_train))\n","print(len(x_test), len(y_test))"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["max_frame_count = getMaxFrameCount(filenames)\n","train_generator = DataGenerator(x_train,\n","                                y_train,\n","                                batch_size,\n","                                max_frame_count,\n","                                frame_height,\n","                                frame_width,\n","                                n_channels)\n","test_generator = DataGenerator(x_test,\n","                               y_test,\n","                               batch_size,\n","                               max_frame_count,\n","                               frame_height,\n","                               frame_width,\n","                               n_channels)\n","len(train_generator), len(test_generator)"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["input_shape = (None, frame_width, frame_height, n_channels)\n","if args.epochs:\n","    epochs = args.epochs\n","else:\n","    epochs = 1\n","class_weight = {\n","    0: 0.33,\n","    1: 0.67\n","}\n","\n","model = Sequential()\n","model.add(ConvLSTM2D(filters=64, kernel_size=(4, 4),\n","                     input_shape=input_shape,\n","                     batch_size=batch_size, data_format='channels_last',\n","                     padding='same', return_sequences=False,\n","                     dropout=0.2, recurrent_dropout=0.3))\n","model.add(BatchNormalization())\n","# model.add(ConvLSTM2D(filters=32, kernel_size=(3, 3),\n","#                      padding='same', return_sequences=True,\n","#                      dropout=0.2, recurrent_dropout=0.3))\n","# model.add(BatchNormalization())\n","# model.add(ConvLSTM2D(filters=32, kernel_size=(3, 3),\n","#                      padding='same', return_sequences=False,\n","#                      dropout=0.2, recurrent_dropout=0.3))\n","# model.add(BatchNormalization())\n","model.add(Flatten())\n","model.add(Dropout(0.3))\n","# model.add(Dense(128, activation='relu'))\n","# model.add(Dense(64, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adadelta')\n","model.summary()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["hist = model.fit_generator(generator=train_generator,\n","                           steps_per_epoch=(len(x_train) // batch_size),\n","                           epochs=1,\n","                           verbose=1,\n","                           validation_data=test_generator,\n","                           validation_steps=(len(x_test) // batch_size),\n","                           class_weight=class_weight,\n","                           use_multiprocessing=False)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Appendix\n","\n","\n","def padEmptyFrames(vid, max_frame_count):\n","    num_frames = max_frame_count - vid.shape[0]\n","    empty_frames = np.empty(\n","        (num_frames, vid.shape[1], vid.shape[2]), dtype='uint8')\n","    padded_vid = np.vstack((vid, empty_frames))\n","\n","    return padded_vid\n","\n","\n","def getFrames(filepath, max_frame_count, pad_frames=True):\n","    # print(filepath)\n","    cap = cv2.VideoCapture(filepath)\n","    vid = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if ret:\n","            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","            # can try going bigger up to 720*1080 later\n","            # gray = cv2.resize(gray, (224, 224))\n","            gray = cv2.resize(gray, (20, 20))\n","            vid.append(gray)\n","            if cv2.waitKey(1) & 0xFF == ord('q'):\n","                break\n","        else:\n","            break\n","\n","    cap.release()\n","    vid = np.array(vid)\n","    if pad_frames:\n","        if vid.shape[0] < max_frame_count:\n","            vid = padEmptyFrames(vid, max_frame_count)\n","\n","    return vid\n","\n","\n","m = getMaxFrameCount(filenames)\n","batch_size = 10\n","idx = 0\n","# x = filenames[idx * batch_size:(idx + 1) * batch_size]\n","# y = labels[idx * batch_size:(idx + 1) * batch_size]\n","x = filenames\n","y = labels\n","\n","x = np.array([getFrames(file, m) for file in tqdm(x)]) / 255\n","# x = x.reshape(self.batch_size, self.max_frame_count, 224, 224, 1)\n","b, f, w, h = x.shape\n","x = x.reshape(b, f, w, h, 1)\n","print(x.shape)"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["y = np.array(y)\n","x.shape, y.shape\n","hist = model.fit(x=x, y=y,\n","                 epochs=1,\n","                 verbose=1,\n","                 class_weight=class_weight)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":[""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}